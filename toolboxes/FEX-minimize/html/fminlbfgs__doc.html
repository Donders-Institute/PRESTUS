
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>fminlbfgs__doc</title><meta name="generator" content="MATLAB 7.11"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2014-03-13"><meta name="DC.source" content="fminlbfgs__doc.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000;
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in
wide windows. */
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
}

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#3"><tt>fminlbfgs</tt></a></li><li><a href="#4">Usage</a></li><li><a href="#5">Description</a></li><li><a href="#6">Input arguments</a></li><li><a href="#9">Output arguments</a></li><li><a href="#14">Options</a></li><li><a href="#32">Examples</a></li><li><a href="#34">See also</a></li></ul></div>
</pre><h2><tt>fminlbfgs</tt><a name="3"></a></h2><p>Finds a local minimum of a function of several variables.</p><h2>Usage<a name="4"></a></h2><pre>[x,fval,exitflag,output,grad] = fminlbfgs(fun,x0,options)</pre><h2>Description<a name="5"></a></h2><p>This optimizer is developed for image registration methods with large amounts of unknown variables.</p><p>Optimization methods supported: * Quasi Newton Broyden&#8211;Fletcher&#8211;Goldfarb&#8211;Shanno (BFGS) * Limited memory BFGS (L-BFGS) * Steepest Gradient Descent optimization.</p><h2>Input arguments<a name="6"></a></h2><p><b><tt>fun</tt></b></p><p>Function handle or string which is minimized, returning an error value and  optionally the error gradient.</p><p>Note that the speed of this optimizer can be improved by also providing the gradient at X. Write the <tt>fun</tt> function as follows</p><pre class="codeinput"><span class="keyword">function</span> [f, g] = FUN(X)
    f <span class="comment">% = ...          function value calculation at X</span>
    <span class="keyword">if</span> (nargout &gt; 1)
        g <span class="comment">% = ...      gradient calculation at X</span>
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><p><b><tt>x0</tt></b></p><p>Initial values of unknowns can be a scalar, vector or matrix (optional)</p><p><b><tt>options</tt></b></p><p>Structure with optimizer options, made by a struct or <tt>optimset</tt>. Note that <tt>optimset</tt> does not support all input options.</p><h2>Output arguments<a name="9"></a></h2><p><b><tt>x</tt></b></p><p>The found location (values) which minimize the function.</p><p><b><tt>fval</tt></b></p><p>The function value at the solution found.</p><p><b><tt>exitflag</tt></b></p><p>Gives value, which explain why the minimizer stopped. Possible values of <tt>exitflag</tt>, and the corresponding exit conditions are</p><pre>       1: Change in the objective function value was less than the
        specified tolerance TolFun.
        2: Change in x was smaller than the specified tolerance TolX.
        3: Magnitude of gradient smaller than the specified tolerance.
        4: Boundary fminimum reached.
        0: Number of iterations exceeded options.MaxIter or number of
        function evaluations exceeded options.FunEvals.
    -1: Algorithm was terminated by the output function.
       -2: Line search cannot find an acceptable point along the current
        search.</pre><p><b><tt>output</tt></b></p><p>Structure with all important ouput values and parameters</p><p><b><tt>grad</tt></b></p><p>the gradient at this location.</p><h2>Options<a name="14"></a></h2><p><b><tt>'GoalsExactAchieve'</tt></b></p><p>If set to 0, a line search method is used which uses a few function calls to do a good line search. When set to 1 a normal line search method with Wolfe conditions is used (default).</p><p><b><tt>'GradConstr'</tt></b></p><p>Set this variable to true if gradient calls are cpu-expensive (default). If false more gradient calls are used and less function calls.</p><p><b><tt>'HessUpdate'</tt></b></p><p>If set to 'bfgs', Broyden&#8211;Fletcher&#8211;Goldfarb&#8211;Shanno optimization is used (default), when the number of unknowns is larger then 3000 the function will switch to Limited memory BFGS, or if you set it to 'lbfgs'. When set to 'steepdesc', steepest decent optimization is used.</p><p><b><tt>'StoreN'</tt></b></p><p>Number of itterations used to approximate the Hessian, in L-BFGS, 20 is default. A lower value may work better with non smooth functions, because than the Hessian is only valid for a specific position. A higher value is recommend with quadratic equations.</p><p><b><tt>'GradObj'</tt></b></p><p>Set to 'on' if gradient available otherwise finited difference is used.</p><p><b><tt>'Display'</tt></b></p><p>Level of display. 'off' displays no output; 'plot' displays all linesearch results in figures. 'iter' displays output at each iteration; 'final' displays just the final output; 'notify' displays output only if the function does not converge.</p><p><b><tt>'TolX'</tt></b></p><p>Termination tolerance on x, default 1e-6.</p><p><b><tt>'TolFun'</tt></b></p><p>Termination tolerance on the function value, default 1e-6.</p><p><b><tt>'MaxIter'</tt></b></p><p>Maximum number of iterations allowed, default 400.</p><p><b><tt>'MaxFunEvals'</tt></b></p><p>Maximum number of function evaluations allowed, default 100 times the amount of unknowns.</p><p><b><tt>'DiffMaxChange'</tt></b></p><p>Maximum stepsize used for finite difference gradients.</p><p><b><tt>'DiffMinChange'</tt></b></p><p>Minimum stepsize used for finite difference gradients.</p><p><b><tt>'OutputFcn'</tt></b></p><p>User-defined function that an optimization function calls at each iteration.</p><p><b><tt>'rho'</tt></b></p><p>Wolfe condition on gradient (c1 on wikipedia), default 0.01.</p><p><b><tt>'sigma'</tt></b></p><p>Wolfe condition on gradient (c2 on wikipedia), default 0.9.</p><p><b><tt>'tau1'</tt></b></p><p>Bracket expansion if stepsize becomes larger, default 3.</p><p><b><tt>'tau2'</tt></b></p><p>Left bracket reduction used in section phase,   default 0.1.</p><p><b><tt>'tau3'</tt></b></p><p>Right bracket reduction used in section phase, default 0.5.</p><h2>Examples<a name="32"></a></h2><pre class="codeinput">options = optimset(<span class="string">'GradObj'</span>,<span class="string">'on'</span>);
X = fminlbfgs(@myfun,2,options)
</pre><pre class="codeoutput">X =
    4.712388980391753e+000
</pre><p>where myfun is a MATLAB function such as:</p><pre class="codeinput"><span class="keyword">function</span> [f,g] = myfun(x)
    f = sin(x) + 3;
    <span class="keyword">if</span> ( nargout &gt; 1 )
        g = cos(x); <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><h2>See also<a name="34"></a></h2><p><a href="matlab:doc('optimset')">optimset</a>, <a href="matlab:doc('fminsearch')">fminsearch</a>, <a href="matlab:doc('fminbnd')">fminbnd</a>, <a href="matlab:doc('fmincon')">fmincon</a>, <a href="matlab:doc('fminunc')">fminunc</a>, <a href="matlab:doc('@')">@</a>, <a href="matlab:doc('inline')">inline</a>.</p>
</pre><p class="footer"><br>
      Published with MATLAB&reg; 7.11<br></p></div><!--
##### SOURCE BEGIN #####
function fminlbfgs__doc

%% |fminlbfgs|
%
% Finds a local minimum of a function of several variables.

%% Usage
%
%  [x,fval,exitflag,output,grad] = fminlbfgs(fun,x0,options)

%% Description
%
% This optimizer is developed for image registration methods with large
% amounts of unknown variables.
%
% Optimization methods supported:
% * Quasi Newton Broyden–Fletcher–Goldfarb–Shanno (BFGS)
% * Limited memory BFGS (L-BFGS)
% * Steepest Gradient Descent optimization.

%% Input arguments
%
% *|fun|*
%
% Function handle or string which is minimized, returning an error
% value and  optionally the error gradient.
%
% Note that the speed of this optimizer can be improved by also
% providing the gradient at X. Write the |fun| function as follows

function [f, g] = FUN(X)
    f % = ...          function value calculation at X
    if (nargout > 1)
        g % = ...      gradient calculation at X
    end
end

%%
% *|x0|*
%
% Initial values of unknowns can be a scalar, vector or matrix (optional)

%%
% *|options|*
%
% Structure with optimizer options, made by a struct or |optimset|.
% Note that |optimset| does not support all input options.

%% Output arguments
%
% *|x|*
%
% The found location (values) which minimize the function.

%%
% *|fval|*
%
% The function value at the solution found.

%%
% *|exitflag|*
%
% Gives value, which explain why the minimizer stopped. Possible values
% of |exitflag|, and the corresponding exit conditions are
%
%       1: Change in the objective function value was less than the
%          specified tolerance TolFun.
%       2: Change in x was smaller than the specified tolerance TolX.
%       3: Magnitude of gradient smaller than the specified tolerance.
%       4: Boundary fminimum reached.
%       0: Number of iterations exceeded options.MaxIter or number of
%          function evaluations exceeded options.FunEvals.
%      -1: Algorithm was terminated by the output function.
%      -2: Line search cannot find an acceptable point along the current
%          search.

%%
% *|output|*
%
% Structure with all important ouput values and parameters

%%
% *|grad|*
%
% the gradient at this location.
%
%% Options
%
% *|'GoalsExactAchieve'|*
%
% If set to 0, a line search method is used which uses a few function
% calls to do a good line search. When set to 1 a normal line search
% method with Wolfe conditions is used (default).

%%
% *|'GradConstr'|*
%
% Set this variable to true if gradient calls are cpu-expensive
% (default). If false more gradient calls are used and less function
% calls.

%%
% *|'HessUpdate'|*
%
% If set to 'bfgs', Broyden–Fletcher–Goldfarb–Shanno optimization is
% used (default), when the number of unknowns is larger then 3000 the
% function will switch to Limited memory BFGS, or if you set it to
% 'lbfgs'. When set to 'steepdesc', steepest decent optimization is
% used.

%%
% *|'StoreN'|*
%
% Number of itterations used to approximate the Hessian, in L-BFGS,
% 20 is default. A lower value may work better with non smooth functions,
% because than the Hessian is only valid for a specific position. A
% higher value is recommend with quadratic equations.

%%
% *|'GradObj'|*
%
% Set to 'on' if gradient available otherwise finited difference is used.

%%
% *|'Display'|*
%
% Level of display. 'off' displays no output; 'plot' displays all linesearch
% results in figures. 'iter' displays output at each iteration; 'final'
% displays just the final output; 'notify' displays output only if the
% function does not converge.

%%
% *|'TolX'|*
%
% Termination tolerance on x, default 1e-6.

%%
% *|'TolFun'|*
%
% Termination tolerance on the function value, default 1e-6.

%%
% *|'MaxIter'|*
%
% Maximum number of iterations allowed, default 400.

%%
% *|'MaxFunEvals'|*
%
% Maximum number of function evaluations allowed, default 100 times the
% amount of unknowns.

%%
% *|'DiffMaxChange'|*
%
% Maximum stepsize used for finite difference gradients.

%%
% *|'DiffMinChange'|*
%
% Minimum stepsize used for finite difference gradients.

%%
% *|'OutputFcn'|*
%
% User-defined function that an optimization function calls at each
% iteration.

%%
% *|'rho'|*
%
% Wolfe condition on gradient (c1 on wikipedia), default 0.01.

%%
% *|'sigma'|*
%
% Wolfe condition on gradient (c2 on wikipedia), default 0.9.

%%
% *|'tau1'|*
%
% Bracket expansion if stepsize becomes larger, default 3.

%%
% *|'tau2'|*
%
% Left bracket reduction used in section phase, default 0.1.

%%
% *|'tau3'|*
%
% Right bracket reduction used in section phase, default 0.5.


%% Examples

options = optimset('GradObj','on');
X = fminlbfgs(@myfun,2,options)

%%
% where myfun is a MATLAB function such as:
function [f,g] = myfun(x)
    f = sin(x) + 3;
    if ( nargout > 1 )
        g = cos(x); end
end

%% See also
%
% <matlab:doc('optimset') optimset>, <matlab:doc('fminsearch') fminsearch>,
% <matlab:doc('fminbnd') fminbnd>, <matlab:doc('fmincon') fmincon>, <matlab:doc('fminunc') fminunc>, <matlab:doc('@') @>, <matlab:doc('inline') inline>.

end

##### SOURCE END #####
--></body></html>
